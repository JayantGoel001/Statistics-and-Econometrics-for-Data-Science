{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION \n",
    "The main goal of the dimensionality reduction techniques is to reduce the dimensions by removing the redundant and dependent\n",
    "features by transforming the features from a higher dimensional space that may lead to a curse of dimensionality problem, to a space with lower dimensions.\n",
    "LDA stands for Linear Discriminant Analysis. It is used as both multiclass classification algorithms and dimentionality\n",
    "reduction technique.\n",
    "It reduces the number of input features or columns on the given dataset. LDA focuses on maximizing separatability among \n",
    "known categories.\n",
    "LDA is an unsupervised approach which means there is no need for labeling classes of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAT IS DIMENTIONALITY REDUCTION ?\n",
    "The techniques of dimensionality reduction are important in applications of Machine Learning, Data Mining, Bioinformatics, and Information Retrieval. The main agenda is to remove the redundant and dependent features by changing the dataset onto a lower-dimensional space.\n",
    "\n",
    "In simple terms, they reduce the dimensions (i.e. variables) in a particular dataset while retaining most of the data.\n",
    "\n",
    "Multi-dimensional data comprises multiple features having a correlation with one another. We can plot multi-dimensional data in just 2 or 3 dimensions with dimensionality reduction. It allows the data to be presented in an explicit manner which can be easily understood by a layman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW ARE LDA MODELS REPRESENTED\n",
    "\n",
    "The representation of LDA is pretty straight-forward. The model consists of the statistical properties of your data that has been calculated for each class. The same properties are calculated over the multivariate Gaussian in the case of multiple variables. The multivariates are means and covariate matrix.\n",
    "\n",
    "Predictions are made by providing the statistical properties into the LDA equation. The properties are estimated from your data. Finally, the model values are saved to file to create the LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKING\n",
    "The two very basic principles on which LDA works can be summerized into two steps:\n",
    "\n",
    "<ul>\n",
    "    <li> Maximizing distance between means of given classes.</li>\n",
    "    <li> Minimizing variation (which LDA calls as scatter) within each category.</li>\n",
    "</ul>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVANTAGES\n",
    "<ul>\n",
    "    <li> Helps in reducing computational costs for a given classification task. </li>\n",
    "    <li> Helpful in avoiding overfitting by minimizing the error in parameter estimation. </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "### LIMITATIONS\n",
    "<ul>\n",
    "    <li> LDA fails to find the lower dimensional space if the dimensions are much higher than \n",
    "         the number of samples in the data matrix. </li>\n",
    "    <li> <b>LDA produces at most C-1 feature projections:</b> If the classification error estimates establish that more features are needed, some other method must be employed to provide those additional features </li> \n",
    "    <li> <b> LDA is a parametric method since it assumes unimodal Gaussian likelihoods: If the distributions are significantly          non-Gaussian, the LDA projections will not be able to preserve any complex structure of the data that may be needed for          classification </b> </li>\n",
    "    <li> LDA will fail when the discriminatory information is not in the mean, but rather in the variance of the data </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIFFERENCE BETWEEN PCA AND LDA\n",
    "<ul>\n",
    "    <li> PCA is unsupervised algorithm while LDA is supervised algorithm. </li>\n",
    "    <li> The goal of PCA is to maximize variation in the given dataset while LDA focuses on \n",
    "         maximizing separatibility among known categories. </li>\n",
    "    <li> LDA performs better multi-class classification tasks than PCA. However, PCA performs better when the sample size is              comparatively small. An example would be comparisons between classification accuracies that are used in image         classification.</li>\n",
    "</ul>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are the extensions of LDA in case we need to use non-linear discriminant analysis:\n",
    "<ul>\n",
    "    <li><b>Quadratic Discriminant Analysis (QDA):</b> Each class uses its own estimate of variance (or covariance when there are   multiple input variables). </li>\n",
    "    <li><b>Flexible Discriminant Analysis (FDA):</b> Where non-linear combinations of inputs is used such as splines.</li>\n",
    "    <li><b>Regularized Discriminant Analysis (RDA):</b> Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications:\n",
    "<ul>\n",
    "    <li><b>Face Recognition:</b> In the field of Computer Vision, face recognition is a very popular application in which each face is represented by a very large number of pixel values. Linear discriminant analysis (LDA) is used here to reduce the number of features to a more manageable number before the process of classification. Each of the new dimensions generated is a linear combination of pixel values, which form a template. The linear combinations obtained using Fisherâ€™s linear discriminant are called Fisher faces.</li>\n",
    "    <li><b>Medical:</b> In this field, Linear discriminant analysis (LDA) is used to classify the patient disease state as mild, moderate or severe based upon the patient various parameters and the medical treatment he is going through. This helps the doctors to intensify or reduce the pace of their treatment.</li>\n",
    "    <li><b>Customer Identification:</b> Suppose we want to identify the type of customers which are most likely to buy a particular product in a shopping mall. By doing a simple question and answers survey, we can gather all the features of the customers. Here, Linear discriminant analysis will help us to identify and select the features which can describe the characteristics of the group of customers that are most likely to buy that particular product in the shopping mall. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for our problem\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7, n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.3548775  -1.69674567  1.6193882  ... -3.33390362  2.45147541\n",
      "  -1.23455205]\n",
      " [ 2.0204277  -1.62734821 -2.27697377 ... -0.28274722 -7.28166465\n",
      "  -0.91070347]\n",
      " [-1.02400669  1.01276423  1.05505825 ...  3.83923974 -1.63530582\n",
      "   3.96050914]\n",
      " ...\n",
      " [-0.36448581 -0.2996303   2.21875138 ... -1.11303373  3.67576043\n",
      "  -1.44164572]\n",
      " [ 0.05614772  1.87270289 -2.63165761 ... -3.07434527  2.31606352\n",
      "   1.65068838]\n",
      " [ 1.09853247  1.61067335  2.7977282  ... -1.62233539 14.09727916\n",
      "   2.27215759]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 4 4 1 2 7 9 4 6 9 1 3 9 5 6 4 9 2 0 4 4 8 0 9 3 8 6 0 0 7 8 3 8 5 5 9 7\n",
      " 1 3 1 8 7 6 7 4 6 5 6 2 8 3 1 7 0 7 0 4 5 1 6 6 8 3 3 3 2 5 8 0 5 6 2 7 1\n",
      " 3 8 7 2 8 0 6 8 2 9 9 8 2 2 5 6 9 4 6 1 4 9 0 9 7 8 7 2 0 8 8 1 7 9 8 1 6\n",
      " 3 9 2 5 5 3 9 1 1 2 0 8 0 7 2 0 5 0 1 8 0 2 2 1 3 0 2 5 9 3 8 8 7 7 0 4 3\n",
      " 4 0 8 5 3 7 4 4 5 5 0 4 5 1 1 4 3 5 2 6 4 2 1 6 6 9 5 3 7 0 1 5 9 5 4 7 3\n",
      " 9 0 0 1 9 5 2 2 7 4 0 1 2 6 4 3 7 6 8 8 3 0 8 3 0 5 5 1 7 8 6 8 4 1 1 3 1\n",
      " 9 9 3 2 8 1 8 1 7 6 1 1 7 6 5 3 4 1 6 5 2 8 6 5 9 0 6 9 6 2 3 4 8 3 8 4 8\n",
      " 1 0 4 0 6 3 8 4 6 9 2 9 2 7 5 1 6 3 0 6 9 3 7 1 5 5 0 9 4 8 9 2 8 2 9 3 2\n",
      " 3 5 1 8 0 0 6 5 1 3 2 8 1 8 6 7 3 2 5 9 6 2 3 4 2 1 5 4 2 9 5 1 7 1 6 0 2\n",
      " 8 6 1 8 7 8 0 3 0 7 1 0 4 1 4 2 0 8 2 7 9 7 3 5 1 5 1 4 9 0 4 9 5 0 8 9 1\n",
      " 2 9 2 8 4 7 9 7 8 4 9 1 7 8 3 7 3 1 9 6 2 9 4 6 8 1 1 5 6 3 0 3 4 8 7 5 6\n",
      " 9 9 6 4 8 2 6 2 7 0 6 8 0 7 0 1 5 7 3 2 2 3 5 2 1 3 6 9 5 4 3 6 7 9 2 4 2\n",
      " 5 0 2 7 4 5 9 1 3 1 8 6 3 1 1 3 3 7 6 6 5 5 8 7 8 9 5 0 7 4 6 3 9 4 7 4 3\n",
      " 5 7 6 7 6 7 9 7 7 7 7 5 7 5 1 6 3 2 6 5 1 0 6 0 1 5 8 9 6 6 3 6 3 6 0 0 8\n",
      " 9 7 6 4 6 8 3 3 5 2 6 3 3 9 2 8 9 2 5 8 6 1 4 4 6 0 9 6 4 3 4 4 2 0 7 3 3\n",
      " 4 9 0 5 3 6 4 8 3 5 2 5 8 2 1 5 4 2 3 7 8 0 1 4 0 6 8 2 7 4 8 1 4 3 5 0 3\n",
      " 8 3 1 9 9 6 0 8 0 7 1 9 2 7 8 6 0 2 3 8 8 8 2 9 0 3 1 4 3 9 9 2 5 0 3 4 1\n",
      " 3 6 6 2 6 2 2 6 5 4 2 6 3 2 7 2 3 3 3 2 2 2 9 7 9 0 9 0 5 3 6 0 3 8 2 6 3\n",
      " 7 5 0 2 4 8 9 9 4 2 8 3 6 9 6 7 1 0 4 4 4 1 7 9 6 4 9 7 1 8 0 1 8 9 7 5 4\n",
      " 8 3 5 6 6 8 1 2 2 3 0 0 0 9 8 0 3 8 7 9 5 4 6 6 0 1 5 5 1 6 4 7 1 2 0 3 4\n",
      " 0 4 0 7 5 7 0 3 8 3 0 9 7 5 6 2 8 5 2 5 3 7 9 1 0 2 2 1 1 9 2 9 2 8 0 4 5\n",
      " 4 0 1 6 6 5 2 5 0 1 7 6 5 0 0 3 4 2 1 6 6 5 4 3 3 4 9 4 2 3 5 1 4 5 1 7 8\n",
      " 7 0 6 9 5 5 9 2 9 8 1 7 0 1 9 9 9 3 2 5 5 6 2 1 7 4 0 3 5 7 7 7 1 2 2 8 9\n",
      " 1 7 3 9 0 2 1 6 1 4 3 6 6 0 1 3 2 8 4 0 7 4 7 9 8 7 1 6 0 1 4 2 3 5 9 5 7\n",
      " 8 2 0 9 0 0 1 0 6 3 1 9 6 8 2 2 8 9 7 3 4 9 7 4 0 5 4 4 1 7 2 8 4 6 1 8 8\n",
      " 3 4 7 5 7 0 5 8 4 5 8 5 9 6 7 1 5 1 6 9 2 1 9 7 2 4 0 7 3 7 5 4 5 7 8 3 5\n",
      " 2 9 4 0 5 4 9 6 9 5 4 2 1 7 2 3 4 1 7 4 4 8 3 3 7 4 5 4 8 0 7 9 2 7 8 6 8\n",
      " 6]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Shape of inout data\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model\n",
    "def model(n_components=None, solver='svd', shrinkage=None, priors=None,\n",
    "          store_covariance=False, tol=0.0001, covariance_estimator=None):\n",
    "    '''\n",
    "    n_components : int, default=None\n",
    "                   Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction. \n",
    "                   If None, will be set to min(n_classes - 1, n_features). This parameter only affects the transform method.\n",
    "                   \n",
    "    solver : {â€˜svdâ€™, â€˜lsqrâ€™, â€˜eigenâ€™}, default=â€™svdâ€™\n",
    "             Solver to use, possible values:\n",
    "             â€˜svdâ€™: Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features.\n",
    "             â€˜lsqrâ€™: Least squares solution. Can be combined with shrinkage or custom covariance estimator.\n",
    "             â€˜eigenâ€™: Eigenvalue decomposition. Can be combined with shrinkage or custom covariance estimator.\n",
    "      \n",
    "    shrinkage : â€˜autoâ€™ or float, default=None\n",
    "                Shrinkage parameter, possible values:\n",
    "                None: no shrinkage (default).\n",
    "                â€˜autoâ€™: automatic shrinkage using the Ledoit-Wolf lemma.\n",
    "                float between 0 and 1: fixed shrinkage parameter.\n",
    "                This should be left to None if covariance_estimator is used. Note that shrinkage works only with â€˜lsqrâ€™ and â€˜eigenâ€™ solvers.\n",
    "\n",
    "    priors : array-like of shape (n_classes,), default=None\n",
    "             The class prior probabilities. By default, the class proportions are inferred from the training data.\n",
    "\n",
    "    store_covariance : bool, default=False\n",
    "                       If True, explicitely compute the weighted within-class covariance matrix when solver is â€˜svdâ€™. The matrix is always computed\n",
    "                       and stored for the other solvers.\n",
    "\n",
    "    tol : float, default=1.0e-4\n",
    "          Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular \n",
    "          values are non-significant are discarded. Only used if solver is â€˜svdâ€™.\n",
    "\n",
    "    covariance_estimator : covariance estimator, default=None\n",
    "                           If not None, covariance_estimator is used to estimate the covariance matrices instead of relying on the empirical \n",
    "                           covariance estimator (with potential shrinkage). The object should have a fit method and a covariance_ attribute \n",
    "                           like the estimators in sklearn.covariance. if None the shrinkage parameter drives the estimate.\n",
    "      '''\n",
    "    lda = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage, \n",
    "                                     priors=priors, n_components=n_components, store_covariance=store_covariance, \n",
    "                                     tol=tol, covariance_estimator=covariance_estimator)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data to the model\n",
    "lda = model(5)\n",
    "lda.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data \n",
    "data_transformation = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.34250698 -0.410752   -0.05284109 -2.52177124 -2.32197387]\n",
      " [ 0.92569633 -0.92633682 -0.29396574 -0.62144384  1.61682597]\n",
      " [-0.36265323 -0.87103112  1.53812275  0.59888243 -1.39423894]\n",
      " ...\n",
      " [-0.83323633  0.06686996  0.39414469 -0.5877848   0.11590941]\n",
      " [ 0.47329133  1.42040541  0.49439799 -0.05149737 -0.53591346]\n",
      " [-1.04969306  0.27613461 -0.13712968 -1.21293132 -0.22775809]]\n"
     ]
    }
   ],
   "source": [
    "print(data_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Notice the reduction of dimensions\n",
    "print(data_transformation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
